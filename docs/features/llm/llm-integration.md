# LLM 集成实现指南

> 本文档由以下文档合并而成：
- `docs/features/LLM_INTEGRATION.md`
- `docs/features/LLM_TRAINING_PLAN.md`
- `docs/features/LLM_REQUEST_QUEUE_OPTIMIZATION.md`

---

## 来源: LLM_INTEGRATION.md

## 相关文档

- [LLM训练计划](../features/LLM_TRAINING_PLAN.md)
- [LLM请求队列优化](../features/LLM_REQUEST_QUEUE_OPTIMIZATION.md)
- [训练数据收集指南](../features/TRAINING_DATA_GUIDE.md)

## 概述

LLM集成功能：
- 本地Ollama支持
- 智能聊天生成
- 训练数据收集
- 请求队列管理



---

## 来源: LLM_TRAINING_PLAN.md

## 目标

训练大模型生成：
- **只返回一句话**
- **最多10个字**
- **简洁、自然、符合游戏场景**

## 当前状态

### 问题分析

从训练数据收集器收集的样本中，常见问题：

1. **输出过长**：大模型经常返回多句话或长句子
2. **冗余表达**：包含"好的，"、"我觉得，"等冗余开头
3. **过于正式**：使用"根据我的分析"等正式表达
4. **多句话**：一次返回多句话，需要人工选择第一句

### 当前处理方式

通过 `contentProcessor.ts` 进行后处理：
- 移除冗余开头和结尾
- 只选择第一句话
- 截断到最多10个字

**问题**：这是临时方案，应该让大模型直接生成符合要求的内容。

## 训练方案

### 阶段1：数据收集（当前阶段）

**目标**：收集足够的训练样本

**方法**：
1. 使用当前系统收集数据
2. 记录原始输出和处理后的输出
3. 分析常见问题和模式

**时间**：1-2周

**输出**：
- 训练数据集（原始 -> 精简后的配对）
- 问题分析报告
- 常见模式统计

### 阶段2：提示词优化

**目标**：优化 prompt，让大模型直接生成简短内容

**方法**：
1. 分析收集的数据，找出导致长输出的 prompt 模式
2. 修改 system prompt，明确要求：
   - "只返回一句话"
   - "最多10个字"
   - "简洁自然，不要冗余表达"
3. 在 prompt 中添加示例

**示例 prompt 改进**：

```
当前：
你是一个过炸扑克游戏的AI玩家，请生成一句聊天内容。

改进后：
你是一个过炸扑克游戏的AI玩家，请生成一句聊天内容。
要求：
1. 只返回一句话（不要多句）
2. 最多10个字
3. 简洁自然，不要"好的，"、"我觉得，"等冗余开头
4. 符合游戏场景，口语化

示例：
- 好牌！
- 这手不错
- 要不起
- 等等我
```

**时间**：1周

**验证**：
- 测试新 prompt 的输出长度
- 统计平均字数
- 检查是否还需要后处理

### 阶段3：Few-shot 学习

**目标**：通过示例让模型学习简短表达

**方法**：
1. 从训练数据中选择高质量样本（精简效果好、自然流畅的）
2. 在 prompt 中添加这些示例
3. 让模型学习这些示例的风格

**示例格式**：

```
示例1：
输入：玩家出牌后，手牌很好
输出：好牌！

示例2：
输入：玩家要不起
输出：要不起

示例3：
输入：玩家出大牌
输出：这手不错
```

**时间**：1-2周

**验证**：
- 对比添加示例前后的输出质量
- 统计符合要求的比例

### 阶段4：微调训练（可选）

**目标**：使用收集的数据对模型进行微调

**方法**：
1. 准备训练数据集：
   - 输入：游戏场景描述
   - 输出：简短聊天内容（10字以内）
2. 使用 LoRA 或全量微调
3. 训练专门的聊天模型

**数据集格式**：

```json
{
  "instruction": "你是一个过炸扑克游戏的AI玩家，请根据游戏场景生成一句聊天内容（最多10个字）。",
  "input": "玩家出牌后，手牌很好，当前轮次领先",
  "output": "好牌！"
}
```

**时间**：2-4周（取决于数据量和计算资源）

**验证**：
- 测试微调后的模型
- 对比微调前后的输出质量
- 统计符合要求的比例

### 阶段5：持续优化

**目标**：持续收集数据，不断改进

**方法**：
1. 定期导出训练数据
2. 分析新数据中的问题
3. 更新 prompt 或重新训练
4. 迭代改进

**时间**：持续进行

## 训练数据要求

### 数据量

- **最少**：500条高质量样本
- **推荐**：1000-2000条
- **理想**：5000+条

### 数据质量

每条数据应包含：
- 游戏场景描述（输入）
- 原始输出（大模型返回）
- 精简后输出（目标输出）
- 处理统计（长度、减少量等）

### 数据筛选标准

优先选择：
1. 精简效果好（减少50%以上）
2. 精简后自然流畅
3. 符合游戏场景
4. 口语化、简洁

## 评估指标

### 定量指标

1. **平均字数**：目标 ≤ 10字
2. **符合率**：≤ 10字的比例，目标 ≥ 90%
3. **单句率**：只包含一句话的比例，目标 ≥ 95%
4. **精简率**：平均精简百分比，目标 ≥ 30%

### 定性指标

1. **自然度**：是否自然流畅
2. **相关性**：是否贴合游戏场景
3. **多样性**：是否有足够的表达变化

## 实施步骤

### 第1周：数据收集

- [ ] 确保训练数据收集器正常工作
- [ ] 收集至少500条样本
- [ ] 分析数据，找出常见问题

### 第2周：提示词优化

- [ ] 修改 system prompt
- [ ] 添加明确的要求和示例
- [ ] 测试新 prompt 的效果

### 第3-4周：Few-shot 学习

- [ ] 选择高质量样本作为示例
- [ ] 在 prompt 中添加示例
- [ ] 测试和优化

### 第5-8周：微调训练（可选）

- [ ] 准备训练数据集
- [ ] 进行模型微调
- [ ] 测试和评估

### 持续：迭代优化

- [ ] 定期收集新数据
- [ ] 分析问题
- [ ] 持续改进

## 工具和资源

### 数据收集

- `trainingDataCollector.ts` - 自动收集训练数据
- 浏览器控制台 - 查看和导出数据

### 数据分析

- Excel/CSV - 分析训练数据
- Python脚本 - 统计分析（可选）

### 模型训练（如果进行微调）

- Ollama - 本地模型训练
- LoRA - 参数高效微调
- 训练框架 - 根据模型选择

## 预期效果

### 短期（1-2周）

- 通过提示词优化，平均字数减少到15字以内
- 符合10字要求的比例达到60%+

### 中期（1-2月）

- 通过 Few-shot 学习，平均字数减少到12字以内
- 符合10字要求的比例达到80%+

### 长期（3-6月）

- 通过微调训练，平均字数减少到10字以内
- 符合10字要求的比例达到90%+
- 基本不需要后处理

## 注意事项

1. **不要过度精简**：保持自然和流畅
2. **保持多样性**：避免所有输出都相似
3. **场景相关性**：确保内容贴合游戏场景
4. **持续监控**：定期检查输出质量

## 当前临时方案

在训练完成前，使用 `contentProcessor.ts` 进行后处理：
- 只选择第一句话
- 截断到最多10个字
- 移除冗余表达

这个方案可以保证输出符合要求，但理想情况是让大模型直接生成符合要求的内容。



---

## 来源: LLM_REQUEST_QUEUE_OPTIMIZATION.md

## 🔧 优化内容

### 问题分析

从日志看，主要问题是：
1. **LLM API响应时间过长**（27-34秒），导致大量请求堆积
2. **无并发控制**：所有LLM请求同时发送，导致Ollama服务器过载
3. **对骂消息丢失**：虽然生成了，但因为超时或阻塞没有播放
4. **气泡堆积**：大量气泡显示，很久才消失

### 解决方案

#### 1. LLM请求队列和并发控制

**实现**：
- ✅ 添加请求队列：`llmRequestQueue`
- ✅ 并发限制：最多同时2个LLM请求（`MAX_CONCURRENT_LLM_REQUESTS = 2`）
- ✅ 优先级排序：对骂（3）> 事件（2）> 随机（1）
- ✅ 队列长度限制：最多20个请求，超过时丢弃低优先级请求

**修改文件**：
- `src/chat/strategy/LLMChatStrategy.ts`

#### 2. 请求去重和缓存

**实现**：
- ✅ 请求去重：相同prompt只发送一次请求
- ✅ 结果缓存：5秒内相同prompt使用缓存结果
- ✅ 等待机制：如果相同prompt正在处理，等待结果而不是重复请求

#### 3. 超时时间优化

**实现**：
- ✅ LLM请求超时：从60秒降到20秒（`LLM_REQUEST_TIMEOUT = 20000`）
- ✅ 南昌话转换超时：5秒（已实现）
- ✅ 快速失败：超时后立即回退，不等待

#### 4. 气泡清理优化

**实现**：
- ✅ 未开始播放：5秒后自动隐藏
- ✅ 未结束播放：8秒后自动隐藏
- ✅ 播放失败：立即隐藏
- ✅ 无语音配置：2秒后自动隐藏

## 📊 优化效果

### LLM请求管理

**之前**：
- 无并发限制，所有请求同时发送
- 超时时间60秒，导致长时间阻塞
- 无优先级，对骂和随机请求同等处理
- 无去重，相同prompt重复请求

**现在**：
- 并发限制：最多2个同时请求
- 超时时间：20秒（从60秒降到20秒）
- 优先级排序：对骂优先处理
- 请求去重：相同prompt只请求一次
- 结果缓存：5秒内使用缓存

### 请求流程

```
LLM请求
  ↓
检查是否正在处理相同prompt
  ├─> 是 → 等待结果
  └─> 否 → 检查并发数
      ├─> 未满 → 立即执行
      └─> 已满 → 加入队列（按优先级排序）
          ↓
      队列处理
          ├─> 检查缓存
          ├─> 执行请求
          └─> 缓存结果
```

### 优先级

| 类型 | 优先级 | 说明 |
|------|--------|------|
| 对骂 | 3 | 最高优先级，优先处理 |
| 事件 | 2 | 中等优先级 |
| 随机 | 1 | 最低优先级 |

## 🔍 调试日志

### 队列管理

```
[LLMChatStrategy] 加入LLM队列（队列长度: 5，优先级: 3）
[LLMChatStrategy] 使用缓存结果
[LLMChatStrategy] ⚠️ LLM队列已满，丢弃低优先级请求
```

### 响应时间警告

```
[LLMChatStrategy] ⚠️ LLM响应时间过长: 15000 ms，可能导致阻塞
```

### 气泡清理

```
[ChatBubble] 5秒内未开始播放，自动隐藏
[ChatBubble] 8秒内未结束播放，自动隐藏
```

## ⚙️ 配置参数

### LLM请求配置

```typescript
// src/chat/strategy/LLMChatStrategy.ts
private readonly MAX_CONCURRENT_LLM_REQUESTS = 2; // 最多同时2个请求
private readonly LLM_REQUEST_TIMEOUT = 20000; // 20秒超时
private readonly CACHE_TTL = 5000; // 缓存5秒
```

### 队列长度

```typescript
// 队列最多20个请求
if (this.llmRequestQueue.length >= 20) {
  // 丢弃低优先级请求
}
```

## 🚀 性能提升

1. **响应速度**：对骂消息优先处理，更快生成
2. **资源占用**：限制并发数，降低服务器压力
3. **用户体验**：避免阻塞，消息及时显示
4. **稳定性**：请求去重和缓存，减少重复请求

## 📝 注意事项

1. **队列溢出**：如果队列满，低优先级请求会被丢弃
2. **缓存时效**：缓存5秒，超过后重新请求
3. **超时处理**：20秒超时后立即回退到规则策略

---

**更新日期**：2024-12-19



---

