# 训练性能分析

## 📊 当前训练方式

### 训练流程
```
10轮次 × 批次大小5 = 50局完整游戏

每局游戏：
1. 创建完整的Game实例（包含所有调度器、语音服务等）
2. 完整运行游戏到结束（可能几百轮出牌）
3. 每个玩家每轮都要：
   - 调用MCTS算法（50次迭代）
   - 执行出牌逻辑（包含验证、记录等）
   - 处理游戏状态更新
4. 收集决策数据
```

### 性能瓶颈

#### 1. **完整游戏实例**
- 使用完整的 `Game` 类，包含：
  - RoundScheduler（轮次调度器）
  - 语音服务检查
  - 游戏状态更新回调
  - 所有游戏逻辑

#### 2. **MCTS算法耗时**
- 每个决策需要50次MCTS迭代
- 每次迭代包含：
  - 选择（Selection）
  - 扩展（Expansion）
  - 模拟（Simulation）- 可能模拟20层深度
  - 反向传播（Backpropagation）

#### 3. **游戏循环延迟**
- 每轮都有10ms延迟
- 游戏可能持续几百轮
- 50局游戏 × 平均200轮 = 10,000次延迟

#### 4. **完整游戏逻辑**
- 包含所有验证、记录、状态更新
- 包含语音检查（虽然跳过，但仍有检查逻辑）

## ⏱️ 时间估算

假设：
- 每局游戏平均200轮出牌
- 每轮MCTS决策：50次迭代 × 20ms = 1秒
- 游戏逻辑处理：50ms
- 延迟：10ms

单局游戏时间：
- 200轮 × (1秒 + 0.05秒 + 0.01秒) = 212秒 ≈ 3.5分钟

50局游戏：
- 50 × 3.5分钟 = 175分钟 ≈ 3小时

## 🚀 优化方案

### 方案1：简化游戏模拟器（推荐）

创建一个专门用于训练的简化游戏模拟器：
- 移除所有UI相关逻辑
- 移除语音服务检查
- 移除调度器队列
- 直接状态转换，无回调
- 减少MCTS迭代次数（训练模式：10-20次）

**预期加速：10-20倍**

### 方案2：减少MCTS迭代次数

训练模式下：
- 从50次减少到10-20次
- 减少模拟深度（从20层到10层）

**预期加速：3-5倍**

### 方案3：并行训练

使用Web Worker并行运行多局游戏：
- 4个Worker同时运行4局游戏
- 50局游戏 → 13批（每批4局）

**预期加速：4倍**

### 方案4：采样训练

不是每局都完整运行：
- 只运行前N轮（比如前50轮）
- 使用模拟器预测后续结果
- 只收集关键决策点

**预期加速：5-10倍**

## 💡 建议

**立即优化**：
1. 减少MCTS迭代次数（训练模式：10-20次）
2. 移除不必要的延迟
3. 跳过语音服务检查

**中期优化**：
1. 创建简化游戏模拟器
2. 实现并行训练

**长期优化**：
1. 使用采样训练
2. 实现增量训练（只训练新数据）

